# @package _global_
defaults:
  - ../dataset@eval_dataset: msmarco_doc_pairs
  - override /dataset@train_dataset: msmarco_doc_triplets
<<<<<<< HEAD
  - override /loss: triplet_distil_kl
=======
>>>>>>> 76c8d87c15483e427cc00078b46bf3320282a7ec
  - override /model: exact_reranker_qmlp_dmlm_long
exp_name: reranker_exact_qmlp_dmlm_msmarco_doc_ce_3_psg
train_dataset:
  num_psg: 3
eval_dataset:
  num_psgs: 3
<<<<<<< HEAD
  run_path: "outputs/qmlp_dmlm_distil_sentence_transformer_kl_l1_0.0001/msmarco_doc/run_doc_3.trec"
=======
  run_path: "run_files_top200/run_max_score_3.trec"
>>>>>>> 76c8d87c15483e427cc00078b46bf3320282a7ec
training_arguments: 
  per_device_train_batch_size: 64
  max_steps: 150000
  gradient_accumulation_steps: 1
  # dataset@eval_dataset: msmarco_rerank
data_collator:
  _target_: lsr.datasets.multi_psgs_triplets.MutiPSGsTripletsBatching
  tokenizer: ${tokenizer}
  
eval_collator: 
  _target_: lsr.datasets.multi_psgs_pairs.MutiPSGsPairsBatching
  tokenizer: ${tokenizer}

trainer: 
  eval_collator: ${eval_collator}
  eval_dataset: ${eval_dataset}

wandb:
  setup:
    project: lsr-framework-phrase
